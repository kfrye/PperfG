#! /usr/bin/bash
# Author: Benjamin Allan
# Sandia National Laboratories
# 10/23/2017
#
# ldmsd startup script called from sbatch input scripts
# setup environment variables/files for ldmsd and run for user
# This version works with pdsh, and requires a slurm/ldms
# environment variables file pregenerated with srun.
#
# arguments: job_id job_dir ldms_slurmenv plugin_config
#
# job_id: slurm job id
# job_dir: where to dump all the ldms-related files.
# ldms_slurmenv: environment file for ldmsd -- generated by default; may be
#                overridden if another file following the same variable
#                conventions is wanted.
# plugin_config: ldms script commands file to start samplers/(aggs)/storage.
#                generated by default and may be overridden or edited
#                once generated.
#
# Requires:
# a) ldmsd installed from source
# b) the arguments above
#
# Affected by environment variables:
# LDMSD_LOG_LEVEL (unset or one of ERROR,WARNING,INFO,DEBUG,CRITICAL)
# LDMS_PREFIX (set this in your shell environment, or Ben's at Sandia
#   is assumed {which usually fails elsewhere}).
# LDMSAGGD_CONNECTION_RETRY_INTERVAL (default 1000000 (microsec))
# LDMSD_INTERVAL_DEFAULT (default 1000000 (microsec) sampler interval)
# ldms_split_schema (default 0) (see below)
# ldms_prefix (see below)
# ldms_startclean if set 1, wipes out previously existing
#   sampler and env var configuration files named in argv.

# This line must be adjusted to point at your ldms installation
# if you don't set LDMS_PREFIX in your shell.
export LDMS_PREFIX=/usr/projects/workflow_perf/ovis/install
if test -z "$LDMS_PREFIX"; then
	ldms_prefix=/usr/projects/workflow_perf/ovis/install
fi
export ldms_prefix=$LDMS_PREFIX
export prefix=$LDMS_PREFIX

# ldms_split_schema 0 means put all instances of sampler plugin into same
# csv output.
#
# ldms_split_schema 1 means put each data instance output into separate
# csv file, but you cannot have more than 20 such files because of
# store_csv.c limits; this limits node count and samplers used.
if test -z "$ldms_split_schema"; then
	ldms_split_schema=0
fi


# HOSTNAME is propagated by slurm, so we cannot use the shell variable
MYhostname=`hostname`
export MYhostname

####################################################################
# Creates a file in HOME for use with all jobs under ldms
# Once created, it can be adjusted manually and will not be
# overwritten.
# Multiple customized files can be managed for various jobs.
function ldms_default_env {
	# do not edit this function
	if test -f $1; then
		echo "Using existing $1 for ldmsd environment"
		return
	fi
	echo "Warning: generating default ldms environment file $1."
	cat << EOF > $1
#! /bin/false
# this script runs within the start_ldms.sh and requires preset variables:
# ldms_prefix, jdir
# LDMS installation data
. \$ldms_prefix/lib/ovis-lib-configvars-kris.sh
. \$ldms_prefix/lib/ovis-ldms-configvars-kris.sh
export PATH=\${ldms_prefix}/sbin:\${ldms_prefix}/bin:\$PATH
export LD_LIBRARY_PATH=\${ovis_ldms_libdir}:\${ovis_lib_libdir}:\${ovis_ldms_pkglibdir}:\${ovis_lib_pkglibdir}:\$LD_LIBRARY_PATH
export ZAP_LIBPATH=\${ovis_lib_pkglibdir}
export LDMSD_PLUGIN_LIBPATH=\${ovis_ldms_pkglibdir}
# output controls
export TESTDIR=\$jdir
export STOREDIR=\$TESTDIR/store
export SOCKDIR=\$TESTDIR/run/ldmsd/\${SLURM_NODEID}
export LOGDIR=\$TESTDIR/logs
export LDMSD_PIDFILE=\$TESTDIR/run/ldmsd/\${SLURM_NODEID}.pid
export LDMSD_SOCKPATH=\$TESTDIR/run
export LDMS_AUTH_FILE=\$HOME/.ldmsauth.conf
# setup file tree for ldms info
mkdir -p \$STOREDIR \$SOCKDIR \$LOGDIR \$LDMSD_SOCKPATH
# set up security for the daemon
if ! test -f \$LDMS_AUTH_FILE; then
	echo "secretword=\${RANDOM}\${RANDOM}\${RANDOM}\${RANDOM}\${RANDOM}" > \$LDMS_AUTH_FILE
fi
chmod -R go-rwx \$TESTDIR \$LDMS_AUTH_FILE
# set log to ERROR if not already set.
# other values are
if test -z "\$LDMSD_LOG_LEVEL"; then
	LDMSD_LOG_LEVEL=ERROR
fi

if test "0" = "\$SLURM_NODEID"; then
	echo "logs and data stored under \$TESTDIR"
fi
EOF

}

####################################################################
# Creates a file in job start dir for use with all jobs under ldms
# Once created, it can be adjusted manually and will not be
# overwritten.
# Multiple customized files can be managed for various jobs.
function ldms_default_plugins_separate {
	# edit this function carefully. better to edit the output instead
	if test -f $1; then
		echo "Using existing ldms plugin config file $1."
		return
	fi
	echo "Warning: generating default ldms plugin config file $1."
	if test -z "$LDMSD_INTERVAL_DEFAULT"; then
		LDMSD_INTERVAL_DEFAULT=1000000
	fi
	cat << EOF > $1

## /proc/meminfo memory metric
load name=meminfo
config name=meminfo producer=\${MYhostname} instance=\${SLURM_NODEID}/meminfo.\${SLURM_NODEID} schema=meminfo.\${SLURM_NODEID} component_id=\${SLURM_NODEID}
start name=meminfo interval=${LDMSD_INTERVAL_DEFAULT} offset=0

## /proc/stat cpu metrics
load name=procstat
config name=procstat producer=\${MYhostname} instance=\${SLURM_NODEID}/procstat.\${SLURM_NODEID} schema=procstat.\${SLURM_NODEID} component_id=\${SLURM_NODEID}
start name=procstat interval=${LDMSD_INTERVAL_DEFAULT} offset=0

## nfs metrics; uncomment if nfs present
#load name=procnfs
#config name=procnfs producer=\${MYhostname} instance=\${SLURM_NODEID}/procnfs.\${SLURM_NODEID} schema=procnfs.\${SLURM_NODEID} component_id=\${SLURM_NODEID}
#start name=procnfs interval=${LDMSD_INTERVAL_DEFAULT}

## interrupt metrics; uncomment if wanted.
#load name=procinterrupts
#config name=procinterrupts producer=\${MYhostname} instance=\${SLURM_NODEID}/procinterrupts.\$SLURM_NODEID schema=procinterrupts.\${SLURM_NODEID} component_id=\${SLURM_NODEID}
#start name=procinterrupts interval=${LDMSD_INTERVAL_DEFAULT}

## ethernet metrics, need interface names 'ifaces' corrected
#load name=procnetdev
#config name=procnetdev producer=\${MYhostname} instance=\${SLURM_NODEID}/procnetdev.\$SLURM_NODEID schema=procnetdev.\${SLURM_NODEID} component_id=\${SLURM_NODEID} ifaces=eth0
#start name=procnetdev interval=${LDMSD_INTERVAL_DEFAULT}

# can add your own stuff here if wanted.

EOF
}

####################################################################
# Creates a file in job start dir for use with all jobs under ldms
# Once created, it can be adjusted manually and will not be
# overwritten.
# Multiple customized files can be managed for various jobs.
# This version puts all sets from the same plugin in the same schema,
# so you cannot have schema defined differently on different nodes.
function ldms_default_plugins_merged {
	# edit this function carefully. better to edit the output instead
	if test -f $1; then
		echo "Using existing ldms plugin config file $1."
		return
	fi
	echo "Warning: generating default ldms plugin config file $1."
	if test -z "$LDMSD_INTERVAL_DEFAULT"; then
		LDMSD_INTERVAL_DEFAULT=1000000
	fi
	cat << EOF > $1

## /proc/meminfo memory metric
load name=meminfo
config name=meminfo producer=\${MYhostname} instance=\${SLURM_NODEID}/meminfo schema=meminfo component_id=\${SLURM_NODEID}
start name=meminfo interval=${LDMSD_INTERVAL_DEFAULT} offset=0

## /proc/stat cpu metrics
load name=procstat
config name=procstat producer=\${MYhostname} instance=\${SLURM_NODEID}/procstat schema=procstat component_id=\${SLURM_NODEID}
start name=procstat interval=${LDMSD_INTERVAL_DEFAULT} offset=0

## nfs metrics; uncomment if nfs present
#load name=procnfs
#config name=procnfs producer=\${MYhostname} instance=\${SLURM_NODEID}/procnfs schema=procnfs component_id=\${SLURM_NODEID}
#start name=procnfs interval=${LDMSD_INTERVAL_DEFAULT}

## interrupt metrics; uncomment if wanted.
#load name=procinterrupts
#config name=procinterrupts producer=\${MYhostname} instance=\${SLURM_NODEID}/procinterrupts schema=procinterrupts component_id=\${SLURM_NODEID}
#start name=procinterrupts interval=${LDMSD_INTERVAL_DEFAULT}

## ethernet metrics, need interface names 'ifaces' corrected
#load name=procnetdev
#config name=procnetdev producer=\${MYhostname} instance=\${SLURM_NODEID}/procnetdev schema=procnetdev component_id=\${SLURM_NODEID} ifaces=eth0
#start name=procnetdev interval=${LDMSD_INTERVAL_DEFAULT}

# can add your own stuff here if wanted.

EOF
}

####################################################################
# Creates a suffix file in job start dir for use with node 0 as aggregator.
# Multiple customized files can be managed for various jobs.
function ldms_job_agg_store_separate {
	if test -f $1; then
		echo "Warning: overwriting ldms agg/store config file $1."
	fi
	if test -z "$LDMSAGGD_CONNECTION_RETRY_INTERVAL"; then
		LDMSAGGD_CONNECTION_RETRY_INTERVAL=1000000
	fi
	if test -z "$LDMSD_INTERVAL_DEFAULT"; then
		LDMSD_INTERVAL_DEFAULT=1000000
	fi
	cat << EOF > $1
# direct each schema on each node to its own store file (general case)
loglevel level=INFO
loglevel level=DEBUG
load name=store_csv
config name=store_csv action=init path=${STOREDIR}

EOF
# add storage policy for each set on each host independently (general case).
#
# If all hosts created all schemas equally, the outer loop
# could go away. This is not the general case, however.
#
j=0
for i in `scontrol show "hostnames=$SLURM_NODELIST"`; do
	echo "# host $i samplers" >> $1
	for s in $sampler_plugins; do
cat  << EOF >> $1
strgp_add name=$s.$j plugin=store_csv container=store_csv schema=$s.$j
strgp_prdcr_add name=$s.$j regex=$i
strgp_start name=$s.$j

EOF
	done
	j=$((j + 1))
done

for i in `scontrol show "hostnames=$SLURM_NODELIST"`; do
	cat  << EOF >> $1
# add $i producer and updater loop to aggregator for
prdcr_add name=$i host=$i type=active xprt=$XPRT interval=$LDMSAGGD_CONNECTION_RETRY_INTERVAL port=$NETPORT
prdcr_start name=$i

updtr_add name=$i interval=$LDMSD_INTERVAL_DEFAULT offset=300000
updtr_prdcr_add name=$i regex=$i
updtr_start name=$i

EOF
done

}

####################################################################
# Creates a suffix file in job start dir for use with node 0 as aggregator.
# Multiple customized files can be managed for various jobs.
# Schema must be same on each compute node for the same sampler plugin.
function ldms_job_agg_store_merged {
	if test -f $1; then
		echo "Warning: overwriting ldms agg/store config file $1."
	fi
	if test -z "$LDMSAGGD_CONNECTION_RETRY_INTERVAL"; then
		LDMSAGGD_CONNECTION_RETRY_INTERVAL=1000000
	fi
	if test -z "$LDMSD_INTERVAL_DEFAULT"; then
		LDMSD_INTERVAL_DEFAULT=1000000
	fi
	cat << EOF > $1
# direct each plugin to common per-plugin store file (isomorphic case)
loglevel level=INFO
loglevel level=DEBUG
load name=store_csv
config name=store_csv action=init path=${STOREDIR}

EOF
# add storage policy for each set on each host independently (general case).
#
# If all hosts created all schemas equally, the outer loop
# could go away. This is not the general case, however.
#
for s in $sampler_plugins; do
	echo "# plugin $s samplers" >> $1
	cat  << EOF >> $1
strgp_add name=$s plugin=store_csv container=store_csv schema=$s
strgp_prdcr_add name=$s regex=\*
strgp_start name=$s
EOF
done

for i in `scontrol show "hostnames=$SLURM_NODELIST"`; do
	cat  << EOF >> $1
# add $i producer and updater loop to aggregator for
prdcr_add name=$i host=$i type=active xprt=$XPRT interval=$LDMSAGGD_CONNECTION_RETRY_INTERVAL port=$NETPORT
prdcr_start name=$i

updtr_add name=$i interval=$LDMSD_INTERVAL_DEFAULT offset=300000
updtr_prdcr_add name=$i regex=$i
updtr_start name=$i

EOF
done

}

##################################################################
# grope the load name=plugin statements out of the sampler script
# excluding comments
function find_sampler_plugins {
	if test -z "$1"; then
		return 1
	fi
	if ! test -f $1; then
		return 1
	fi
	sampler_plugins=`grep "load name=" $1 |grep -v '.*#.*load name=' |grep -v store | cut -d' ' -f 2 |sed -e 's/name=//g' -e 's/;.*//g'` 2>/dev/null
	echo $sampler_plugins
}

#################### main #########################################
#

jobnum=$1
jdir=$2
lenv=$3
dconfig=$4
export jdir

if test -z "$jobnum"; then
	echo "$0: need job id as first argument and work dir as second"
	exit 1
fi

if test -z "$jdir"; then
	echo "$0: need job ldms data directory as second argument"
	exit 1
fi

# load pregenerated slurm-propagated environment values
slurmenvfile=$jdir/slurm-pdsh-envs/$MYhostname
if ! test -f $slurmenvfile; then
	echo missing $slurmenvfile. srun getslurminfo.sh first in batch script.
	exit 1
fi
. $slurmenvfile

if test -z "$SLURM_NODEID"; then
	echo "$0: did not get slurm variables from $slurmenvfile"
	exit 1
fi

if test "0" = "$SLURM_NODEID"; then
	if test -n "ldms_startclean"; then
		echo "Warning: starting with all fresh ldmsd files".
		/bin/rm -rf $3 $4 \
			$jdir/store \
			$jdir/run \
			$jdir/logs \
			$jdir/vg.*.log \
			$jdir/.ldmsenv \
			$jdir/agg_ls.sh \
			$jdir/samp_ls.sh

	fi
	# generate default env, if not already present
	ldms_default_env $lenv
else
	sleep 2
fi


if ! test -f $lenv; then
	echo $0: unable to find/create ldms environment file $lenv
	exit 1
fi

# load ldmsd run environment variables
. $lenv
(cd $jdir; cp $lenv .ldmsenv)

echo $MYhostname $SLURM_NODEID

# This number of threads may need adjustment if running an aggregator
NUMTHREADS=4

# This number of threads may need adjustment
AGGNUMTHREADS=8

# fixed port here means only one ldmsd collector per node can run
NETPORT=60411
AGGPORT=$(( $NETPORT + 1 ))
XPRT=sock
AGGXPRT=$XPRT

allconfig=$SOCKDIR/all-config.local

# generate default config if name not given or is of missing file
if test -z "$dconfig"; then
	dconfig=$SOCKDIR/samp.config
	if test "$ldms_split_schema" = "0"; then
		ldms_default_plugins_merged $dconfig
	else
		ldms_default_plugins_separate $dconfig
	fi
else
	if ! test -f $dconfig; then
		if test "$ldms_split_schema" = "0"; then
			ldms_default_plugins_merged $dconfig
		else
			ldms_default_plugins_separate $dconfig
		fi
	fi
fi

if test "0" = "$SLURM_NODEID"; then
	aggconfig=$SOCKDIR/agg-config.gen
	sampler_plugins=$(find_sampler_plugins $dconfig)
	export sampler_plugins
	if test -z "$sampler_plugins"; then
		echo "no sampler plugins found loaded in $dconfig"
		exit 1
	fi

	if test "$ldms_split_schema" = "0"; then
		ldms_job_agg_store_merged $aggconfig
	else
		ldms_job_agg_store_separate $aggconfig
	fi
	n_sampler_plugins=$(wc -w <<< "$sampler_plugins")
	export n_sampler_plugins
fi

cat $dconfig > $allconfig

logfile=$LOGDIR/ldmsd.$MYhostname.log.$SLURM_NODEID
env > $logfile.env
echo "ldmsd starting on $MYhostname"
# to debug with valgrind, uncomment the valgrind line and 
# join with ldmsd start line.
# valgrind -v --log-file=$jdir/vg.$MYhostname.log --trace-children=yes
ldmsd -c $allconfig \
	-a $LDMS_AUTH_FILE \
	-x $XPRT:$NETPORT \
	-P $NUMTHREADS \
	-l $logfile \
	-r $LDMSD_PIDFILE \
	-v $LDMSD_LOG_LEVEL &> $logfile.start
# tinkers can add other options here by inserting continuation lines
# before the final redirect
echo "ldmsd started on $MYhostname"
sleep 2 ; # give collectors time to start
# start the aggregator
if test "0" = "$SLURM_NODEID"; then
	if test -z "$LDMSD_MEM_SZ"; then
		# man page suggestion is
		# LDMSD_MEM_SZ="$(($n_sampler_plugins * $SLURM_NNODES *4))k"
		# We assume array researchers are more likely with slurm
		# serious folk will set LDMSD_MEM_SZ in the launch environment
		LDMSD_MEM_SZ="$(( $n_sampler_plugins * $SLURM_NNODES ))M"
	fi
	export LDMSD_MEM_SZ
	# dump stuff for ls in dot file
	scontrol show "hostnames=$SLURM_NODELIST" > $LDMSD_SOCKPATH/nodelist
	nodelist=`cat $LDMSD_SOCKPATH/nodelist`
	echo $nodelist
	(cd $jdir; cat << EOF > agg_ls.sh
#! /usr/bin/bash
# job was $SLURM_JOBID
export PATH=$PATH
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH
set -x
ldms_ls -h $MYhostname -p $AGGPORT -x $AGGXPRT -m $LDMSD_MEM_SZ -a $LDMS_AUTH_FILE \$*
EOF

cat << EOF > samp_ls.sh
#! /usr/bin/bash
# job was $SLURM_JOBID
export PATH=$PATH
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH
set -x
EOF

for i in $nodelist; do
	cat << EOF >> samp_ls.sh
ldms_ls -h $i -p $NETPORT -x $XPRT -a $LDMS_AUTH_FILE \$*
EOF
done
chmod u+rx agg_ls.sh samp_ls.sh
)
	alogfile=$LOGDIR/ldmsd.$SLURM_NODEID.agg.log
	echo "ldms-aggd starting on $MYhostname"
	# to debug with valgrind, uncomment the valgrind line and 
	# join with ldmsd start line.
	# valgrind -v --log-file=$jdir/vg.agg.log --trace-children=yes
	ldms-aggd \
		-r $LDMSD_PIDFILE.agg \
		-l $alogfile \
		-c $aggconfig \
		-m $LDMSD_MEM_SZ \
		-a $LDMS_AUTH_FILE \
		-x $AGGXPRT:$AGGPORT \
		-P $AGGNUMTHREADS \
		-v $LDMSD_LOG_LEVEL &> $alogfile.start
	# tinkers can add other options here by inserting continuation lines
	# before the final redirect
	echo "ldms-aggd started on $MYhostname"
fi
exit 0
